{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b519ad4",
   "metadata": {},
   "source": [
    "## Building Logistic Regression from scratch in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9b42e",
   "metadata": {},
   "source": [
    "![Picture4.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAMAAABEK3PtAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAzUExURQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKMFRskAAAAQdFJOUwATIi88SlhndoWWqLjL3O5QTd6UAAAACXBIWXMAABcRAAAXEQHKJvM/AAAEFklEQVR4Xu3ci3KjIBQGYEGDiAi8/9Pu4ZJNE3Cjkm3F/t/MjiaTpo3+Hi6S7QAAAAAAAAAAAAAAAOATuNJpD+AIppxDhuA4Ju1skCGocJt5J5EhqIQMQS1kCGohQ1ALGYJayBDUQoagFjIEtZAhqIUMQS1kCGohQ1CpN84NaR9gP3eHGF2dcgtLuwBHjG5GpwVqCKfCP4CD+tCQKSfSYwCAb3YfMzHr+rADsJNK3WnMBcJRPBYiKkOYw4GDVBiSjStlSKZJwkSmpwG+oELEu86slCFkCDbwhUigNwQVfCFa9vaGUl36pPTO0CJ14F5HOu+flN4ZWkSFCIMyqKPckvYy6FP/OH8Kzj/9+4/5RWQoGtIBIPUlW/u3oa3fbLjXLZxNeyeGOeq3pFs4bZh1c3yiinHWv9uNIrRh4dbwkd/5n/1ohqy/Gp2z514FN8e/Tzvzib9TxM7DLQ+HLJS50Y1p78R+MkM8tI/a3eLDs7JhaQy17B8ZfDDnKIrM5MtHSxlSLdwNP5whJqtbauF/dSOrl/qPdQhn/4F1aNCelTJkGugO+cWwaW8fSe1Q2q0ytrGKkm39fwF9B9xvWWiyqHhlIRB0xKdSeSlkiNFrb4szV1yaI8xCA4z04Cu9M1j+gLZAxZ7wQxhfPTzO/hjaKp8l/2gqRM86Uay9hQzd3KhGeq+T9xiP4Hq4H6QXbzL0mC6IDUNv2/hWCY2iXjptqxlKK/umeHhkIS1qpVksZIiGhL5PvZy9y3jQoQy9YLaNC4zbHS1uXI1lwpKIbjbhuSfjypxPIUM6ljF90cm5HRkSdNGaQvVmi22joV/2fAlP+ygIO/kNL5ShnnqSzx/b96G++NL6pQL36zPEtBv9sCY/C2GE0i2nr9PUAu/IejjhWoaqMuU3kpiZX8cRqxnq042oqy403ZyhJSQlXJ3PlJ8/Y6oQrnOhT7pnok9ShoRlkn6IF869XhgvN2Z5Wza6yW946QK8gq0ZknHkZbJL+X71FXoMp1Ie1q/3qSlDjFpuQVHSIQJPlG++y33kPENzfN1lVwm+ZGjtBm363lHDh2FOI2v59AnWMyTcLKkJGpwc85sjcVRfnpTLM5RWKa8tVm7exgwJqjP8ppxutRqLNKzvY7vy3uCMpXPuN6na0LGJO/RkCA8du8LhyDKUvndzdBr4/Da2ZZNPlJ6avZBoFBWyM9itczR0YMJPpA35myF6s3Ag6CWFgVaWIX9vv6OOVbPX3zsbM9T6sHQOZTXYeibT/f3HfMA9Qzca1fve32Do3fJBf5ahUQv69Uv+yqv4JRn6Rnl/6Oo2ZsiPdEl/0ZEFVOBUZQsVJssQt4Z3XJamqeFX+zuyfY1RlqGup7CZKV8tA1CWZwgAAAAAAAAAAAAAAGC7rvsDkSM8e6OA8XgAAAAASUVORK5CYII=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc59583",
   "metadata": {},
   "source": [
    "$\\hat{{Y}}$ --> predicted value\n",
    "\n",
    "X --> Input Variable\n",
    "\n",
    "w --> weight\n",
    "\n",
    "b --> bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886cb3e",
   "metadata": {},
   "source": [
    "**Gradient Descent:**\n",
    "\n",
    "Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
    "\n",
    "w  =  w - α*dw\n",
    "\n",
    "b  =  b - α*db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c726d",
   "metadata": {},
   "source": [
    "**Learning Rate:**\n",
    "\n",
    "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39982979",
   "metadata": {},
   "source": [
    "**Derivatives:**\n",
    "    \n",
    "![Picture3.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84278e",
   "metadata": {},
   "source": [
    "## Importing the Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cee178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d0e20",
   "metadata": {},
   "source": [
    "## Scratch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e6167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "\n",
    "  # declaring learning rate & number of iterations (Hyperparametes)\n",
    "  def __init__(self, learning_rate, no_of_iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_iterations = no_of_iterations\n",
    "\n",
    "  # fit function to train the model with dataset\n",
    "  def fit(self, X, Y):\n",
    "        # number of data points in the dataset (number of rows)  -->  m\n",
    "        # number of input features in the dataset (number of columns)  --> n\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        #initiating weight & bias value\n",
    "        self.w = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        # implementing Gradient Descent for Optimization\n",
    "        for i in range(self.no_of_iterations):\n",
    "            self.update_weights()\n",
    "\n",
    "  def update_weights(self):\n",
    "        # Y_hat formula (sigmoid function)\n",
    "        Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))    \n",
    "    \n",
    "        # derivaties\n",
    "        dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))\n",
    "        db = (1/self.m)*np.sum(Y_hat - self.Y)\n",
    "    \n",
    "        # updating the weights & bias using gradient descent\n",
    "        self.w = self.w - self.learning_rate * dw\n",
    "        self.b = self.b - self.learning_rate * db\n",
    "\n",
    "  # Sigmoid Equation & Decision Boundary\n",
    "  def predict(self, X):\n",
    "        Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) )) \n",
    "        Y_pred = np.where( Y_pred > 0.5, 1, 0)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a0eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
